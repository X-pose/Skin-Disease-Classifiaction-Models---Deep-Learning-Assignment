{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+M/EAlhk5yrRB0bV0hzlH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/X-pose/Skin-Disease-Classifiaction-Models---Deep-Learning-Assignment/blob/IT21223594---Thalangama-T.P/VGG16_Skin_disease_classification_model_IT21223594.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**VGG16-Skin disease classification model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Author : Thalangama T.P.\n",
        "\n",
        "IT Number : IT21223594"
      ],
      "metadata": {
        "id": "K4aTypBIBOpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset mounting and data extraction\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OxE0lqB_B6g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#extract dataset----------------------------------------------------------------------------------------\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to dataset zip file in Google Drive\n",
        "zip_file_path = '/content/drive/My Drive/Skin Disease.v1i.multiclass.zip'\n",
        "\n",
        "# Directory to extract the files\n",
        "extract_dir = '/content/Extracted_data'\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"Dataset extracted to {extract_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNV0uBS9CXNN",
        "outputId": "48a2fcdb-59d0-4197-c897-65e0b6526739"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset extracted to /content/Extracted_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data splitting\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### The original dataset contained only training and test sets. For validation purposes, I have allocated 10% of the training data to create a separate validation set. The following code implements the logic for splitting the data and creating the validation set."
      ],
      "metadata": {
        "id": "rlReeVdECxdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrR7tN1yAysu",
        "outputId": "a6da85f7-1e12-4b01-e1d7-069a8ddb1619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 3674\n",
            "Validation set size: 409\n",
            "Test set size: 454\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up paths for train, test, and validation data\n",
        "data_dir = Path('/content/Extracted_data')\n",
        "train_images_dir = data_dir / 'train'\n",
        "test_images_dir = data_dir / 'test'\n",
        "val_images_dir = data_dir / 'valid'\n",
        "\n",
        "# Validating the validation directory existence\n",
        "val_images_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load the CSV file with annotations\n",
        "train_annotations_path = train_images_dir / '_classes.csv'\n",
        "train_df = pd.read_csv(train_annotations_path)\n",
        "\n",
        "# Load test data CSV\n",
        "test_annotations_path = test_images_dir / '_classes.csv'\n",
        "test_df = pd.read_csv(test_annotations_path)\n",
        "\n",
        "# Split 10% of the training data into validation\n",
        "train_filenames = train_df['filename'].tolist()\n",
        "random.shuffle(train_filenames)\n",
        "split_index = int(0.9 * len(train_filenames))\n",
        "train_split = train_filenames[:split_index]\n",
        "val_split = train_filenames[split_index:]\n",
        "\n",
        "# Create a new DataFrame for validation data\n",
        "val_df = train_df[train_df['filename'].isin(val_split)]\n",
        "train_df = train_df[train_df['filename'].isin(train_split)]\n",
        "\n",
        "# Move validation images to the validation folder\n",
        "for val_filename in val_split:\n",
        "    val_img_path = train_images_dir / val_filename\n",
        "    shutil.move(str(val_img_path), val_images_dir / val_filename)\n",
        "\n",
        "# Save the updated train and validation CSV files\n",
        "train_df.to_csv(train_images_dir / '_classes.csv', index=False)\n",
        "val_df.to_csv(val_images_dir / '_classes.csv', index=False)\n",
        "\n",
        "# Optionally print the number of images in each set\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataloader setup\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### The following codebase contains the logic related to applying transforms and data augmentation to the split datasets. Train data uses more data augmentation techniques (for better generalization of the model) while validation and test datasets use the basic transforms to mimic real-world scenarios. Then data are loaded into respective DataLoaders to be used in the later parts of the training."
      ],
      "metadata": {
        "id": "MulejbgLDsP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class SkinDiseaseDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.classes = list(self.data_frame.columns[1:])  # Skip 'filename' column\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data_frame.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        labels = torch.tensor(self.data_frame.iloc[idx, 1:].values.astype('int'), dtype=torch.float32)\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, labels\n",
        "\n",
        "# Defining Transformations\n",
        "# More data augmentation added to help with better generalization\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224 as per VGG16 default input size\n",
        "    transforms.RandomHorizontalFlip(),  # Default augmentation: horizontal flip\n",
        "    transforms.ColorJitter(brightness=0.2, saturation=0.2),  # Data Augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Default normalization for VGG16\n",
        "])\n",
        "\n",
        "#Contains only the basic transform to mimic real world data\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Paths to CSVs and Image Directories\n",
        "train_csv = '/content/Extracted_data/train/_classes.csv'\n",
        "train_img_dir = '/content/Extracted_data/train/'\n",
        "\n",
        "valid_csv = '/content/Extracted_data/valid/_classes.csv'\n",
        "valid_img_dir = '/content/Extracted_data/valid/'\n",
        "\n",
        "\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = SkinDiseaseDataset(csv_file=train_csv, img_dir=train_img_dir, transform=train_transforms)\n",
        "valid_dataset = SkinDiseaseDataset(csv_file=valid_csv, img_dir=valid_img_dir, transform=test_transforms)\n",
        "\n",
        "\n",
        "# Create DataLoaders. In here I am using smaller batch sizes for faster convergence\n",
        "train_loader = DataLoader(train_dataset, batch_size= 8, shuffle=True, num_workers=8)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
        "\n",
        "\n",
        "\n",
        "# Checking DataLoader Output\n",
        "for images, labels in train_loader:\n",
        "    print(images.size(), labels.size())\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdRnf1rJEPnW",
        "outputId": "0bd2fc76-9f47-4acc-85d2-67ab3f1316c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3, 224, 224]) torch.Size([8, 17])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model codebase\n",
        "\n",
        "\n",
        "---\n",
        "#### The following codebase creates a classification model using a pre-trained VGG16 network. It freezes featuer extracting layers, and modifies the classifier for 17 skin diseases classes.\n"
      ],
      "metadata": {
        "id": "AraQPvRuFkHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.vgg import vgg16\n",
        "\n",
        "def create_classification_model(num_classes=17):  # 17 skin diseases\n",
        "    # Loading the pre-trained VGG16 model without the top classifier layers\n",
        "    model = vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # Freeze the featuer extractor convolutional base\n",
        "    for param in model.features.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    # Modifying classifier layers\n",
        "    model.classifier = torch.nn.Sequential(\n",
        "      torch.nn.Flatten(),\n",
        "      torch.nn.Linear(25088, 1024),  # First hidden layer\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Dropout(0.5),\n",
        "\n",
        "      torch.nn.Linear(1024, 512),    # Second hidden layer\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Dropout(0.4),\n",
        "\n",
        "      torch.nn.Linear(512, 256),     # Third hidden layer\n",
        "      torch.nn.ReLU(),\n",
        "\n",
        "      torch.nn.Linear(256, num_classes),  # Final output layer\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = create_classification_model(num_classes=17)\n",
        "    print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdJmrkD2F13w",
        "outputId": "c8de4a0a-1418-48cf-bc89-fdd99926ab9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=25088, out_features=1024, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.4, inplace=False)\n",
            "    (7): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (8): ReLU()\n",
            "    (9): Linear(in_features=256, out_features=17, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss function, optimizer, and scheduler.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In here I am trying to address the class imbalance problems that may included in the dataset by assigning higher weights to less frequent classes. Then after normalizing the class weights (So that they sum to 1 to stabalize the training) and passing the modified weights as paramters to the loss function.\n",
        "\n",
        "I have selected CrossEntropyLoss for multiclass image classification due to its ability to handle multiple classes, provide effective probabilistic output comparisons, penalize incorrect predictions, and ensure stable gradient computation during training.\n",
        "\n",
        "As my optimizer, I have selected AdamW with a learning rate of 0.00001 due to its ability to combine the benefits of Adam’s adaptive learning rate with weight decay (L2 regularization), which helps prevent overfitting, ensures stable convergence, and maintains better generalization performance, in deep learning models.\n",
        "\n",
        "For better learning, I have used a learning rate scheduler, ReduceLROnPlateau(), with a patience of 3 to monitor the loss improvement, as it automatically reduces the learning rate when the loss plateaus, helping to fine-tune the model and avoid overshooting or stagnation during training."
      ],
      "metadata": {
        "id": "q6tNQw9tHn-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for optimization and scheduling\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Calculate the class counts from the training dataset.\n",
        "class_counts = np.sum(train_dataset.data_frame.iloc[:, 1:].values, axis=0)\n",
        "\n",
        "# Set a smoothing factor to prevent division by zero or extremely small weights for underrepresented classes.\n",
        "smoothing_factor = 1.0\n",
        "\n",
        "# Calculate class weights inversely proportional to the class counts, adding the smoothing factor to each count.\n",
        "# This helps deal with class imbalance by assigning higher weights to less frequent classes.\n",
        "class_weights = 1.0 / torch.tensor(class_counts + smoothing_factor, dtype=torch.float)\n",
        "\n",
        "# Normalize the class weights so that they sum to 1, which stabilizes the training.\n",
        "class_weights = class_weights / torch.sum(class_weights)\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "#Using the CrossEntropyLoss with modified weight to address class imbalance problem\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Using the AdamW (Adam with weight decay), as the optimizer with a learning rate of 0.00001.\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.00001)\n",
        "\n",
        "# Use a learning rate scheduler to reduce the learning rate when the loss stops improving.\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "VA0ronFdH_ba"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and validation loop\n",
        "\n",
        "---\n",
        "\n",
        "####Training and validation are both performed within the same code block. Validation is carried out after each training epoch to assess the model's performance on the validation set, tracking both training and validation accuracy, as well as monitoring training and validation loss throughout the process. A graph that show the loss over epoch is displayed at the end of the process.\n"
      ],
      "metadata": {
        "id": "aa4TGux9M2Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Training and validation loop for classification task\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Training loop parameters\n",
        "num_epochs = 15\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "class_names = ['Actinic', 'Atopic', 'Benign', 'Candidiasis', 'Dermatitis', 'Dermatofibroma', 'Melanocytic', 'Melanoma', 'Ringworm', 'Squamous', 'Tinea', 'Vascular', 'Carcinoma', 'Cell', 'Keratosis', 'Lesion', 'Nevus']\n",
        "\n",
        "# Function for training one epoch\n",
        "def train_one_epoch(model, train_loader, optimizer, criterion, device, class_names):\n",
        "    model.train()  # Setting the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update running loss\n",
        "        running_loss += loss.item() * images.size(0)  # Total loss for the batch\n",
        "\n",
        "        # Accuracy calculation\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        labels_single = torch.argmax(labels, dim=1)  # Convert to single-label representation\n",
        "\n",
        "\n",
        "        correct_preds += (predicted == labels_single).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)  # Mean loss for the epoch\n",
        "    accuracy = correct_preds / total_preds  # Accuracy for the epoch\n",
        "\n",
        "    return epoch_loss, accuracy\n",
        "\n",
        "\n",
        "# Function for validation (without gradient updates)\n",
        "def validate(model, valid_loader, criterion, device,threshold=0.5):\n",
        "    model.eval()  # Setting model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(valid_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)  # Total loss for the batch\n",
        "\n",
        "            # Softmax to get predicted probabilities\n",
        "            predicted_probs = torch.softmax(outputs, dim=-1)\n",
        "\n",
        "            # Apply threshold logic to get the predicted class\n",
        "            predicted_classes = (predicted_probs > threshold).long()\n",
        "\n",
        "            # Convert labels to single-label representation\n",
        "            labels_single = torch.argmax(labels, dim=1)\n",
        "\n",
        "            # Accuracy calculation\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            # Convert labels to single-label representation\n",
        "            labels_single = torch.argmax(labels, dim=1)\n",
        "\n",
        "            #Predicted class vs actual class.\n",
        "            for i in range(len(predicted)):\n",
        "              pred_class = class_names[predicted[i].item()]  # Get predicted class name\n",
        "              actual_class = class_names[labels_single[i].item()]  # Get actual class name\n",
        "\n",
        "            correct_preds += (predicted == labels_single).sum().item()\n",
        "\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(valid_loader.dataset)  # Mean loss for the epoch\n",
        "    accuracy = correct_preds / total_preds  # Accuracy for the epoch\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step(epoch_loss)  # Pass the validation loss to the scheduler\n",
        "\n",
        "    return epoch_loss, accuracy\n",
        "\n",
        "# Training loop with progress visualization\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, class_names)\n",
        "\n",
        "    # Validate after each epoch\n",
        "    valid_loss, valid_acc = validate(model, valid_loader, criterion, device)\n",
        "\n",
        "    # Store loss values for visualization\n",
        "    train_loss_history.append(train_loss)\n",
        "    valid_loss_history.append(valid_loss)\n",
        "\n",
        "    # Print stats\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n",
        "\n",
        "# Visualize loss after training is completed\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_history, label='Training Loss')\n",
        "plt.plot(valid_loss_history, label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mxIq492bOEwF",
        "outputId": "00b2c8e8-1a91-4ab8-a4a5-ab6dc2249217"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:07<00:00, 60.11it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 61.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3007, Train Acc: 0.1824\n",
            "Valid Loss: 0.2429, Valid Acc: 0.3081\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 75.43it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 64.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2080, Train Acc: 0.3704\n",
            "Valid Loss: 0.1589, Valid Acc: 0.5721\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 74.64it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 62.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1623, Train Acc: 0.4652\n",
            "Valid Loss: 0.1347, Valid Acc: 0.7213\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 75.57it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 63.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1423, Train Acc: 0.5128\n",
            "Valid Loss: 0.1219, Valid Acc: 0.7457\n",
            "Epoch 5/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 74.20it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 64.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1288, Train Acc: 0.5621\n",
            "Valid Loss: 0.1133, Valid Acc: 0.6161\n",
            "Epoch 6/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 74.08it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 61.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1201, Train Acc: 0.5920\n",
            "Valid Loss: 0.1062, Valid Acc: 0.7653\n",
            "Epoch 7/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 74.43it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 62.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1125, Train Acc: 0.6037\n",
            "Valid Loss: 0.1021, Valid Acc: 0.8778\n",
            "Epoch 8/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 75.22it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 60.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1070, Train Acc: 0.6271\n",
            "Valid Loss: 0.0982, Valid Acc: 0.8289\n",
            "Epoch 9/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 74.55it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 63.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1019, Train Acc: 0.6467\n",
            "Valid Loss: 0.0953, Valid Acc: 0.8166\n",
            "Epoch 10/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 73.44it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 63.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0988, Train Acc: 0.6432\n",
            "Valid Loss: 0.0936, Valid Acc: 0.7873\n",
            "Epoch 11/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 73.86it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 61.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0963, Train Acc: 0.6508\n",
            "Valid Loss: 0.0921, Valid Acc: 0.7286\n",
            "Epoch 12/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:05<00:00, 76.89it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 63.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0940, Train Acc: 0.6611\n",
            "Valid Loss: 0.0910, Valid Acc: 0.7579\n",
            "Epoch 13/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 73.78it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 62.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0925, Train Acc: 0.6598\n",
            "Valid Loss: 0.0899, Valid Acc: 0.7433\n",
            "Epoch 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 75.31it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 58.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0907, Train Acc: 0.6562\n",
            "Valid Loss: 0.0894, Valid Acc: 0.7824\n",
            "Epoch 15/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460/460 [00:06<00:00, 74.13it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 63.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0902, Train Acc: 0.6758\n",
            "Valid Loss: 0.0890, Valid Acc: 0.7286\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the model.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "####Model is saved after each training session so that model can be used later for further training or evaluation steps."
      ],
      "metadata": {
        "id": "qcQn6xY9Ookz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_save_path = '/content/drive/My Drive/skin_disease_model_complete.pth'\n",
        "torch.save(model, model_save_path)\n",
        "print(f\"Entire model saved successfully to: {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8C-_nPiO-vK",
        "outputId": "c1c7e075-d8b0-46c2-f144-7494d8e7620a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entire model saved successfully to: /content/drive/My Drive/skin_disease_model_complete.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data loading for testing"
      ],
      "metadata": {
        "id": "m2A064CQQIj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#extract dataset----------------------------------------------------------------------------------------\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to dataset zip file in Google Drive\n",
        "zip_file_path = '/content/drive/My Drive/Skin Disease.v1i.multiclass.zip'\n",
        "\n",
        "# Directory to extract the files\n",
        "extract_dir = '/content/Extracted_data'\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"Dataset extracted to {extract_dir}\")\n",
        "\n",
        "data_dir = Path('/content/Extracted_data')\n",
        "\n",
        "test_images_dir = data_dir / 'test'\n",
        "\n",
        "# Load test data CSV\n",
        "test_annotations_path = test_images_dir / '_classes.csv'\n",
        "test_df = pd.read_csv(test_annotations_path)\n",
        "\n",
        "\n",
        "class SkinDiseaseDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.classes = list(self.data_frame.columns[1:])  # Skip 'filename' column\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data_frame.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        labels = torch.tensor(self.data_frame.iloc[idx, 1:].values.astype('int'), dtype=torch.float32)\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, labels\n",
        "\n",
        "#Contains only the basic transform to mimic real world data\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "#Paths to CSVs and image files\n",
        "test_csv = '/content/Extracted_data/test/_classes.csv'\n",
        "test_img_dir = '/content/Extracted_data/test/'\n",
        "\n",
        "#Create datasets\n",
        "test_dataset = SkinDiseaseDataset(csv_file=test_csv, img_dir=test_img_dir, transform=test_transforms)\n",
        "\n",
        "#Create dataloaders\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emn18zkHQOPF",
        "outputId": "c05d9407-675c-4828-f065-8efbacbd77c0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset extracted to /content/Extracted_data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the model\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "####The model is being loaded for continuous training or evaluation"
      ],
      "metadata": {
        "id": "TFvXLTb_PLOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the entire model from the saved path\n",
        "model_save_path = '/content/drive/My Drive/skin_disease_model_complete.pth'\n",
        "model = torch.load(model_save_path)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_5LYSWLPElo",
        "outputId": "f122d4ad-5cf1-4289-ede1-ca39f94e6c62"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-250978e7cf04>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(model_save_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model performance metrics\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Macro averaged F1 Score metric:"
      ],
      "metadata": {
        "id": "CwtRhs3GT65J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#F1 score for model\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class_names = test_df.columns[1:].tolist()\n",
        "\n",
        "\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "def evaluate_f1(model, data_loader, device, class_names):\n",
        "    \"\"\"Evaluates the model using F1 score for multi-class classification.\"\"\"\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for images, labels in tqdm(data_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            probabilities = torch.sigmoid(outputs) # Sigmoid for multi-label probabilities\n",
        "\n",
        "            # Threshold predictions to get binary labels. Considering the most confident probabilities\n",
        "            predicted = (probabilities > 0.92).int()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())  # Move labels to CPU and convert to numpy\n",
        "            all_predictions.extend(predicted.cpu().numpy())  # Move predictions to CPU and convert to numpy\n",
        "\n",
        "    # Calculate F1 score for each class\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=1)  # Macro-averaged F1 score\n",
        "\n",
        "    return f1\n",
        "\n",
        "# Determine device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "f1_score_macro = evaluate_f1(model, test_loader, device, class_names)\n",
        "print(f\"Macro-averaged F1 Score: {f1_score_macro:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXB1g9flUHiX",
        "outputId": "2cfd555d-c786-421e-acf4-d5b161a00613"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 57/57 [00:04<00:00, 13.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Macro-averaged F1 Score: 0.8756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Accuracy metric:"
      ],
      "metadata": {
        "id": "TAoTeoFjVbmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy of the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def calculate_model_accuracy(all_predictions, all_labels):\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "\n",
        "    if all_predictions.ndim > 1:\n",
        "        all_predictions = np.argmax(all_predictions, axis=1)\n",
        "    if all_labels.ndim > 1:\n",
        "        all_labels = np.argmax(all_labels, axis=1)\n",
        "\n",
        "    # Use accuracy_score from sklearn to compute the accuracy\n",
        "    accuracy = accuracy_score(all_labels, all_predictions) * 100  # To get percentage accuracy\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Assuming all_predictions and all_labels are numpy arrays or tensors containing the indices of the predicted/true class\n",
        "accuracy = calculate_model_accuracy(all_predictions, all_labels)\n",
        "\n",
        "print(f\"Overall Model Accuracy: {accuracy:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJFI0DBWVmtT",
        "outputId": "61de29d4-7638-43d2-d80a-5d1008ee3352"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Model Accuracy: 81.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top K-accuracy metric:"
      ],
      "metadata": {
        "id": "7sT-Jz0MW_1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def top_k_accuracy_multilabel(output, target, topk=(1, 5)):\n",
        "    \"\"\"\n",
        "    Computes the accuracy over the k top predictions for multi-label classification\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        # Get the top k predictions for each sample\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "\n",
        "        # Convert target to list of true label indices for each sample\n",
        "        true_labels = [\n",
        "            (target[i] == 1).nonzero(as_tuple=False).squeeze().tolist()\n",
        "            for i in range(batch_size)\n",
        "        ]\n",
        "\n",
        "        # Ensure true_labels are always lists\n",
        "        true_labels = [\n",
        "            [label] if isinstance(label, int) else label\n",
        "            for label in true_labels\n",
        "        ]\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = sum(\n",
        "                any(label in pred[i][:k].tolist() for label in sample_labels)\n",
        "                for i, sample_labels in enumerate(true_labels)\n",
        "            )\n",
        "            res.append(correct_k * (100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "# Assuming all_predictions and all_labels are numpy arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Convert numpy arrays to torch tensors\n",
        "all_predictions_tensor = torch.from_numpy(all_predictions)\n",
        "all_labels_tensor = torch.from_numpy(all_labels)\n",
        "\n",
        "# Calculate Top-1 and Top-5 accuracy\n",
        "top1_accuracy, top5_accuracy = top_k_accuracy_multilabel(all_predictions_tensor, all_labels_tensor, topk=(1, 5))\n",
        "\n",
        "print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dINHeKA3XbVu",
        "outputId": "2c86202e-fce9-4b1d-8f13-e1aed9655877"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 Accuracy: 85.02%\n",
            "Top-5 Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification report:\n"
      ],
      "metadata": {
        "id": "YJrbizz-YL14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Convert predictions to binary values based on a threshold (e.g., 0.5)\n",
        "threshold = 0.5\n",
        "all_predictions_binary = np.array(all_predictions) > threshold\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(np.array(all_labels), all_predictions_binary, target_names=class_names)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyAy-EdiYau0",
        "outputId": "43d4ddd2-7f39-49c4-dce9-b1f3b2bdd812"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Actinic       0.57      1.00      0.72        51\n",
            "         Atopic       1.00      1.00      1.00        45\n",
            "         Benign       0.88      1.00      0.93        56\n",
            "    Candidiasis       0.98      1.00      0.99        60\n",
            "     Dermatitis       1.00      1.00      1.00        45\n",
            " Dermatofibroma       0.68      1.00      0.81        54\n",
            "    Melanocytic       0.65      1.00      0.79        39\n",
            "       Melanoma       0.58      1.00      0.73        44\n",
            "       Ringworm       0.98      1.00      0.99        60\n",
            "       Squamous       0.63      1.00      0.77        54\n",
            "          Tinea       0.98      1.00      0.99        60\n",
            "       Vascular       0.86      1.00      0.93        51\n",
            "      carcinoma       0.64      1.00      0.78        54\n",
            "           cell       0.61      1.00      0.76        54\n",
            "      keratosis       0.99      1.00      1.00       107\n",
            "         lesion       0.84      1.00      0.91        51\n",
            "          nevus       0.63      1.00      0.77        39\n",
            "\n",
            "      micro avg       0.78      1.00      0.87       924\n",
            "      macro avg       0.79      1.00      0.88       924\n",
            "   weighted avg       0.81      1.00      0.89       924\n",
            "    samples avg       0.83      1.00      0.89       924\n",
            "\n"
          ]
        }
      ]
    }
  ]
}